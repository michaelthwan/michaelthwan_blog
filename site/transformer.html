<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding the Transformer</title>
    
    <!-- Meta -->
    <meta name="description" content="Understanding the building blocks and design choices of the Transformer architecture.">
    
    <!-- Fonts: Inter + JetBrains Mono -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });"></script>
    
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <!-- Header -->
    <header class="site-header">
        <div class="header-content l-page">
            <a href="index.html" class="logo">
                <svg viewBox="-607 419 64 64" class="logo-icon">
                    <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
                </svg>
                Knowledge Blog
            </a>
            <nav class="header-nav">
                <a href="#">About</a>
                <a href="#">Archive</a>
            </nav>
        </div>
    </header>

    <!-- Title Section -->
    <header class="d-title-section">
        <h1 class="d-title">Understanding the Transformer</h1>
        <p class="d-subtitle">Understanding the building blocks and design choices of the Transformer architecture.</p>
        
        <!-- Byline Grid -->
        <div class="d-byline">
            <div class="d-byline-column">
                <div class="d-byline-heading">Authors</div>
                <div class="d-byline-content">
                    Ashish Vaswani<br>
                    Noam Shazeer<br>
                    Niki Parmar<br>
                    <em>et al.</em>
                </div>
            </div>
            <div class="d-byline-column">
                <div class="d-byline-heading">Affiliations</div>
                <div class="d-byline-content">
                    Google Brain<br>
                    Google Research
                </div>
            </div>
            <div class="d-byline-column">
                <div class="d-byline-heading">Published</div>
                <div class="d-byline-content">
                    June 12, 2017
                </div>
            </div>
            <div class="d-byline-column">
                <div class="d-byline-heading">DOI</div>
                <div class="d-byline-content">
                    <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Layout -->
    <div class="d-article-container">
        <!-- Table of Contents -->
        <nav class="d-toc">
            <div class="d-toc-sticky">
                <h2 class="d-toc-title">Contents</h2>
                <ul class="d-toc-list">
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#sequential-bottleneck">The Sequential Bottleneck</a></li>
                    <li class="toc-indent"><a href="#no-parallel">Lack of Parallelization</a></li>
                    <li class="toc-indent"><a href="#long-range">Long-Range Dependencies</a></li>
                    <li><a href="#attention-lookup">Attention as a Lookup</a></li>
                    <li><a href="#scaled-attention">Scaled Dot-Product Attention</a></li>
                    <li><a href="#multi-head">Multi-Head Attention</a></li>
                    <li><a href="#architecture">The Transformer Architecture</a></li>
                    <li class="toc-indent"><a href="#encoder">Encoder</a></li>
                    <li class="toc-indent"><a href="#decoder">Decoder</a></li>
                    <li><a href="#positional">Positional Encoding</a></li>
                    <li><a href="#why-attention">Why Self-Attention?</a></li>
                    <li><a href="#training">Training and Results</a></li>
                </ul>
            </div>
        </nav>

        <!-- Article Content -->
        <article class="d-article">
            
            <!-- Intro note -->
            <p class="d-note">
                This article explains the landmark paper 
                <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> 
                by Vaswani et al. (2017), which introduced the Transformer architecture 
                that powers GPT, BERT, and nearly every modern language model.
            </p>

            <!-- Introduction -->
            <section id="introduction">
                <h2>Introduction</h2>
                <p>
                    The dominant sequence transduction models are based on complex recurrent or 
                    convolutional neural networks that include an encoder and a decoder. The best 
                    performing models also connect the encoder and decoder through an attention mechanism.
                </p>
                <p>
                    The Transformer is a new architecture based solely on attention mechanisms, 
                    dispensing with recurrence and convolutions entirely. Experiments show these 
                    models to be superior in quality while being more parallelizable and requiring 
                    significantly less time to train.
                </p>
            </section>

            <!-- Sequential Bottleneck -->
            <section id="sequential-bottleneck">
                <h2>The Sequential Bottleneck</h2>
                
                <p>
                    Before the Transformer, the dominant approach to sequence tasks—machine translation, 
                    language modeling, text generation—was the <strong>recurrent neural network (RNN)</strong>, 
                    particularly LSTMs and GRUs.
                </p>

                <p>
                    RNNs process sequences one token at a time. To compute the hidden state at position 
                    $t$, you need the hidden state at position $t-1$:
                </p>

                <div class="d-math-block">
                    $$h_t = f(h_{t-1}, x_t)$$
                </div>

                <p>This creates two fundamental problems:</p>

                <h3 id="no-parallel">Lack of Parallelization</h3>
                
                <p>
                    Because each step depends on the previous step, you cannot parallelize computation 
                    within a single sequence. Training is inherently sequential in time. For long 
                    sequences, this becomes a severe bottleneck.
                </p>

                <figure class="d-figure">
                    <div class="d-figure-content">
                        <div class="diagram-flow">
                            <div class="flow-row">
                                <span class="flow-label">Token:</span>
                                <div class="flow-items">
                                    <span class="flow-item">x₁</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item">x₂</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item">x₃</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item">x₄</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item">x₅</span>
                                </div>
                            </div>
                            <div class="flow-row">
                                <span class="flow-label">Hidden:</span>
                                <div class="flow-items">
                                    <span class="flow-item highlight">h₁</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item highlight">h₂</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item highlight">h₃</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item highlight">h₄</span>
                                    <span class="flow-arrow">→</span>
                                    <span class="flow-item highlight">h₅</span>
                                </div>
                            </div>
                            <div class="flow-annotation">↑ must wait for previous</div>
                        </div>
                    </div>
                    <figcaption class="d-figure-caption">
                        RNNs process tokens sequentially. Each hidden state depends on the previous one, 
                        preventing parallel computation.
                    </figcaption>
                </figure>

                <h3 id="long-range">Long-Range Dependencies</h3>
                
                <p>
                    Information from early tokens must survive many sequential steps to influence later 
                    processing. Gradients must flow backward through all those steps. In practice, this 
                    makes learning long-range dependencies difficult, even with gating mechanisms like LSTM.
                </p>

                <div class="d-callout">
                    <strong>Key question:</strong> Can we design an architecture where every position can 
                    directly attend to every other position—without sequential dependencies?
                </div>

                <p>The answer is the <strong>Transformer</strong>.</p>
            </section>

            <!-- Attention as Lookup -->
            <section id="attention-lookup">
                <h2>Attention as a Lookup</h2>

                <p>
                    The core idea of attention is surprisingly simple: it's a <strong>soft lookup</strong> 
                    into a set of values, where the lookup key determines how much weight to give each value.
                </p>

                <p>Think of it like a database query:</p>

                <ul>
                    <li>You have a <strong>query</strong> (what you're looking for)</li>
                    <li>You have a set of <strong>keys</strong> (labels for stored items)</li>
                    <li>You have a set of <strong>values</strong> (the stored items themselves)</li>
                </ul>

                <p>
                    The attention mechanism compares your query to each key, computes a relevance score, 
                    and returns a weighted combination of the values.
                </p>

                <figure class="d-figure">
                    <div class="d-figure-content">
                        <div class="attn-viz">
                            <div class="attn-row">
                                <span class="attn-label c-query">Query:</span>
                                <span class="attn-desc">"What information is relevant here?"</span>
                            </div>
                            <div class="attn-row">
                                <span class="attn-label c-key">Keys:</span>
                                <div class="attn-items">
                                    <span class="attn-item">k₁</span>
                                    <span class="attn-item">k₂</span>
                                    <span class="attn-item">k₃</span>
                                    <span class="attn-item">k₄</span>
                                    <span class="attn-item">k₅</span>
                                </div>
                            </div>
                            <div class="attn-row">
                                <span class="attn-label c-value">Values:</span>
                                <div class="attn-items">
                                    <span class="attn-item">v₁</span>
                                    <span class="attn-item highlight">v₂</span>
                                    <span class="attn-item">v₃</span>
                                    <span class="attn-item">v₄</span>
                                    <span class="attn-item">v₅</span>
                                </div>
                            </div>
                            <div class="attn-row">
                                <span class="attn-label">Scores:</span>
                                <div class="attn-scores">
                                    <span class="attn-score low">0.1</span>
                                    <span class="attn-score high">0.7</span>
                                    <span class="attn-score low">0.05</span>
                                    <span class="attn-score low">0.1</span>
                                    <span class="attn-score low">0.05</span>
                                </div>
                            </div>
                            <div class="attn-output">
                                <span class="attn-label">Output:</span>
                                <span class="attn-output-eq">0.1·v₁ + <strong>0.7·v₂</strong> + 0.05·v₃ + 0.1·v₄ + 0.05·v₅</span>
                            </div>
                        </div>
                    </div>
                    <figcaption class="d-figure-caption">
                        Attention computes a weighted sum of values, where weights come from comparing 
                        a query to keys. Here, key k₂ matches the query best, so v₂ dominates the output.
                    </figcaption>
                </figure>

                <div class="d-callout">
                    <strong>Key insight:</strong> Attention connects any two positions in constant time. 
                    There's no sequential path that information must traverse.
                </div>
            </section>

            <!-- Scaled Dot-Product Attention -->
            <section id="scaled-attention">
                <h2>Scaled Dot-Product Attention</h2>

                <p>
                    The Transformer uses a specific form of attention called 
                    <strong>Scaled Dot-Product Attention</strong>.
                </p>

                <p>Given:</p>
                <ul>
                    <li><strong>Queries</strong> $Q \in \mathbb{R}^{n \times d_k}$ — what we're looking for</li>
                    <li><strong>Keys</strong> $K \in \mathbb{R}^{m \times d_k}$ — what we're looking in</li>
                    <li><strong>Values</strong> $V \in \mathbb{R}^{m \times d_v}$ — what we retrieve</li>
                </ul>

                <p>The attention output is:</p>

                <div class="d-equation-panel">
                    <div class="d-equation-title">Scaled Dot-Product Attention</div>
                    <div class="d-equation-main">
                        $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
                    </div>
                    <div class="d-equation-legend">
                        <div class="d-legend-item">
                            <span class="d-legend-dot query"></span>
                            <span><strong>Query</strong> $Q$: what information does this position need?</span>
                        </div>
                        <div class="d-legend-item">
                            <span class="d-legend-dot key"></span>
                            <span><strong>Key</strong> $K$: what information does this position offer?</span>
                        </div>
                        <div class="d-legend-item">
                            <span class="d-legend-dot value"></span>
                            <span><strong>Value</strong> $V$: the actual content to retrieve</span>
                        </div>
                        <div class="d-legend-item">
                            <span class="d-legend-dot param"></span>
                            <span><strong>Scaling</strong> $\sqrt{d_k}$: prevents dot products from growing too large</span>
                        </div>
                    </div>
                </div>

                <h3>Step by step</h3>

                <ol>
                    <li>
                        <strong>Compute compatibility scores:</strong> $QK^T$ gives an $n \times m$ matrix 
                        of dot products. Entry $(i, j)$ measures how much query $i$ matches key $j$.
                    </li>
                    <li>
                        <strong>Scale:</strong> Divide by $\sqrt{d_k}$. Without scaling, large $d_k$ values 
                        push dot products into regions where softmax has very small gradients.
                    </li>
                    <li>
                        <strong>Normalize:</strong> Apply softmax row-wise. Each query now has a probability 
                        distribution over keys.
                    </li>
                    <li>
                        <strong>Retrieve:</strong> Multiply by $V$. Each output is a weighted combination 
                        of values.
                    </li>
                </ol>

                <h3>Why scale?</h3>

                <p>
                    For large $d_k$, the dot products $q \cdot k$ tend to have large magnitude (variance 
                    roughly $d_k$). This pushes softmax into saturated regions where gradients vanish. 
                    Scaling by $\sqrt{d_k}$ keeps the variance at 1.
                </p>
            </section>

            <!-- Multi-Head Attention -->
            <section id="multi-head">
                <h2>Multi-Head Attention</h2>

                <p>
                    A single attention function can only focus on one type of relationship at a time. 
                    <strong>Multi-Head Attention</strong> runs multiple attention functions in parallel, 
                    each with its own learned projections.
                </p>

                <div class="d-equation-panel">
                    <div class="d-equation-title">Multi-Head Attention</div>
                    <div class="d-equation-main">
                        $$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O$$
                        <br><br>
                        $$\text{where } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$
                    </div>
                    <div class="d-equation-legend">
                        <div class="d-legend-item">
                            <span class="d-legend-dot param"></span>
                            <span>$W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$: learned projections</span>
                        </div>
                        <div class="d-legend-item">
                            <span class="d-legend-dot param"></span>
                            <span>$W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$: output projection</span>
                        </div>
                    </div>
                </div>

                <p>Each head can learn to attend to different things:</p>

                <ul>
                    <li>One head might focus on the <strong>previous word</strong></li>
                    <li>Another might focus on the <strong>subject of the sentence</strong></li>
                    <li>Another might focus on <strong>semantically similar words</strong></li>
                </ul>

                <p>
                    The paper uses $h = 8$ heads with $d_k = d_v = 64$ (for $d_{\text{model}} = 512$).
                </p>
            </section>

            <!-- Architecture -->
            <section id="architecture">
                <h2>The Transformer Architecture</h2>

                <p>
                    The Transformer follows the encoder-decoder structure, but built entirely from 
                    attention and feed-forward layers.
                </p>

                <figure class="d-figure">
                    <div class="d-figure-content">
                        <div class="arch-diagram">
                            <!-- Encoder -->
                            <div class="arch-column">
                                <div class="arch-title">Encoder</div>
                                <div class="arch-stack">
                                    <div class="arch-layer">
                                        <div class="arch-component attn">Multi-Head<br>Self-Attention</div>
                                        <div class="arch-norm">Add & Norm</div>
                                        <div class="arch-component ffn">Feed Forward</div>
                                        <div class="arch-norm">Add & Norm</div>
                                    </div>
                                    <span class="arch-repeat">×6</span>
                                </div>
                                <div class="arch-embed">
                                    <div class="arch-embed-box">Input Embedding</div>
                                    <span>+</span>
                                    <div class="arch-embed-box pos">Positional Encoding</div>
                                </div>
                                <div class="arch-input-label">Inputs</div>
                            </div>
                            
                            <!-- Arrow -->
                            <div class="arch-arrow">
                                <svg viewBox="0 0 50 20">
                                    <defs>
                                        <marker id="arrowhead" markerWidth="10" markerHeight="7" 
                                                refX="9" refY="3.5" orient="auto">
                                            <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
                                        </marker>
                                    </defs>
                                    <line x1="0" y1="10" x2="40" y2="10" stroke="#666" 
                                          stroke-width="2" marker-end="url(#arrowhead)"/>
                                </svg>
                                <span class="arch-arrow-label">K, V</span>
                            </div>

                            <!-- Decoder -->
                            <div class="arch-column">
                                <div class="arch-title">Decoder</div>
                                <div class="arch-stack">
                                    <div class="arch-layer">
                                        <div class="arch-component attn-masked">Masked Multi-Head<br>Self-Attention</div>
                                        <div class="arch-norm">Add & Norm</div>
                                        <div class="arch-component attn-cross">Multi-Head<br>Cross-Attention</div>
                                        <div class="arch-norm">Add & Norm</div>
                                        <div class="arch-component ffn">Feed Forward</div>
                                        <div class="arch-norm">Add & Norm</div>
                                    </div>
                                    <span class="arch-repeat">×6</span>
                                </div>
                                <div class="arch-embed">
                                    <div class="arch-embed-box">Output Embedding</div>
                                    <span>+</span>
                                    <div class="arch-embed-box pos">Positional Encoding</div>
                                </div>
                                <div class="arch-input-label">Outputs (shifted right)</div>
                            </div>
                        </div>
                    </div>
                    <figcaption class="d-figure-caption">
                        The Transformer architecture. The encoder (left) processes the input sequence. 
                        The decoder (right) generates the output, attending to both itself and the encoder output.
                    </figcaption>
                </figure>

                <section id="encoder">
                    <h3>Encoder</h3>
                    <p>Each encoder layer has two sub-layers:</p>
                    <ol>
                        <li><strong>Multi-head self-attention:</strong> Every position attends to every position</li>
                        <li><strong>Feed-forward network:</strong> Applied independently to each position</li>
                    </ol>
                    <div class="d-math-block">
                        $$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$
                    </div>
                    <p>Residual connections and layer normalization wrap each sub-layer.</p>
                </section>

                <section id="decoder">
                    <h3>Decoder</h3>
                    <p>Each decoder layer has three sub-layers:</p>
                    <ol>
                        <li><strong>Masked self-attention:</strong> Each position attends only to earlier positions</li>
                        <li><strong>Cross-attention:</strong> Queries from decoder; keys/values from encoder</li>
                        <li><strong>Feed-forward network:</strong> Same as encoder</li>
                    </ol>
                </section>
            </section>

            <!-- Positional Encoding -->
            <section id="positional">
                <h2>Positional Encoding</h2>

                <p>
                    Self-attention is permutation-equivariant—it has no notion of position. 
                    The Transformer adds <strong>positional encodings</strong> to the input embeddings.
                </p>

                <div class="d-math-block">
                    $$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
                    $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
                </div>

                <figure class="d-figure">
                    <div class="d-figure-content">
                        <div class="pos-viz">
                            <div class="pos-grid" id="pos-encoding-grid"></div>
                            <div class="pos-axis-labels">
                                <span>Position →</span>
                                <span>Dimension →</span>
                            </div>
                        </div>
                    </div>
                    <figcaption class="d-figure-caption">
                        Positional encoding visualization. Each row is a position, each column is a dimension. 
                        Lower dimensions vary slowly; higher dimensions vary quickly.
                    </figcaption>
                </figure>

                <p>
                    For any fixed offset $k$, $PE_{pos+k}$ can be written as a linear function of 
                    $PE_{pos}$. This allows the model to learn to attend by relative position.
                </p>
            </section>

            <!-- Why Self-Attention -->
            <section id="why-attention">
                <h2>Why Self-Attention?</h2>

                <div class="d-table-wrapper">
                    <table class="d-table">
                        <thead>
                            <tr>
                                <th>Layer Type</th>
                                <th>Complexity</th>
                                <th>Sequential Ops</th>
                                <th>Max Path Length</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="highlight-row">
                                <td>Self-Attention</td>
                                <td>$O(n^2 \cdot d)$</td>
                                <td class="good">$O(1)$</td>
                                <td class="good">$O(1)$</td>
                            </tr>
                            <tr>
                                <td>Recurrent</td>
                                <td>$O(n \cdot d^2)$</td>
                                <td class="bad">$O(n)$</td>
                                <td class="bad">$O(n)$</td>
                            </tr>
                            <tr>
                                <td>Convolutional</td>
                                <td>$O(k \cdot n \cdot d^2)$</td>
                                <td class="good">$O(1)$</td>
                                <td>$O(\log_k n)$</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p>
                    Self-attention connects all positions in $O(1)$ sequential operations, enabling 
                    full parallelization. It also provides a direct path between any two positions, 
                    making long-range dependencies easier to learn.
                </p>

                <div class="d-callout warning">
                    <strong>Trade-off:</strong> Self-attention has $O(n^2)$ memory complexity. For 
                    very long sequences, this can become prohibitive.
                </div>
            </section>

            <!-- Training -->
            <section id="training">
                <h2>Training and Results</h2>

                <p><strong>Setup:</strong></p>
                <ul>
                    <li>Data: WMT 2014 English-German (4.5M pairs) and English-French (36M pairs)</li>
                    <li>Hardware: 8 NVIDIA P100 GPUs</li>
                    <li>Time: Base model 12 hours; Big model 3.5 days</li>
                </ul>

                <div class="d-table-wrapper">
                    <table class="d-table">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>EN-DE BLEU</th>
                                <th>EN-FR BLEU</th>
                                <th>Training Cost</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Previous SOTA</td>
                                <td>26.36</td>
                                <td>41.29</td>
                                <td>$7.7 \times 10^{19}$ FLOPs</td>
                            </tr>
                            <tr class="highlight-row">
                                <td><strong>Transformer (big)</strong></td>
                                <td class="good"><strong>28.4</strong></td>
                                <td class="good"><strong>41.8</strong></td>
                                <td class="good">$2.3 \times 10^{19}$ FLOPs</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p>
                    The Transformer achieves state-of-the-art results at a fraction of the training cost.
                </p>
            </section>

            <!-- Bibliography -->
            <section class="d-bibliography">
                <h2>References</h2>
                <ol>
                    <li>
                        Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., 
                        Kaiser, Ł., & Polosukhin, I. (2017). 
                        <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. 
                        NeurIPS 2017.
                    </li>
                    <li>
                        Bahdanau, D., Cho, K., & Bengio, Y. (2014). 
                        Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015.
                    </li>
                    <li>
                        Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer Normalization.
                    </li>
                </ol>
            </section>

            <!-- Appendix -->
            <footer class="d-appendix">
                <p>
                    This article is a Distill-style explanation of the Transformer paper. 
                    <a href="https://arxiv.org/abs/1706.03762">Read the original paper →</a>
                </p>
            </footer>

        </article>
    </div>

    <script src="script.js"></script>
</body>
</html>
