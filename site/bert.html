<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding BERT</title>
    
    <!-- Meta -->
    <meta name="description" content="Pre-training deep bidirectional representations for language understanding.">
    
    <!-- Fonts: Inter + JetBrains Mono -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });"></script>
    
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <!-- Header -->
    <header class="site-header">
        <div class="header-content l-page">
            <a href="index.html" class="logo">
                <svg viewBox="-607 419 64 64" class="logo-icon">
                    <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
                </svg>
                Knowledge Blog
            </a>
            <nav class="header-nav">
                <a href="#">About</a>
                <a href="#">Archive</a>
            </nav>
        </div>
    </header>

    <!-- Title Section -->
    <header class="d-title-section">
        <h1 class="d-title">Understanding BERT</h1>
        <p class="d-subtitle">Pre-training deep bidirectional representations for language understanding.</p>
        
        <!-- Byline Grid -->
        <div class="d-byline">
            <div class="d-byline-column">
                <div class="d-byline-heading">Authors</div>
                <div class="d-byline-content">
                    Jacob Devlin<br>
                    Ming-Wei Chang<br>
                    Kenton Lee<br>
                    Kristina Toutanova
                </div>
            </div>
            <div class="d-byline-column">
                <div class="d-byline-heading">Affiliations</div>
                <div class="d-byline-content">
                    Google AI Language
                </div>
            </div>
            <div class="d-byline-column">
                <div class="d-byline-heading">Published</div>
                <div class="d-byline-content">
                    October 11, 2018
                </div>
            </div>
            <div class="d-byline-column">
                <div class="d-byline-heading">DOI</div>
                <div class="d-byline-content">
                    <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Layout -->
    <div class="d-article-container">
        <!-- Table of Contents -->
        <nav class="d-toc">
            <div class="d-toc-sticky">
                <h2 class="d-toc-title">Contents</h2>
                <ul class="d-toc-list">
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#directionality">The Directionality Problem</a></li>
                    <li><a href="#bidirectional-challenge">The Bidirectional Challenge</a></li>
                    <li><a href="#mlm">Masked Language Modeling</a></li>
                    <li><a href="#nsp">Next Sentence Prediction</a></li>
                    <li><a href="#architecture">BERT Architecture</a></li>
                    <li class="toc-indent"><a href="#model-sizes">Model Sizes</a></li>
                    <li class="toc-indent"><a href="#input-repr">Input Representation</a></li>
                    <li><a href="#pretraining">Pre-training</a></li>
                    <li><a href="#finetuning">Fine-tuning BERT</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#ablations">Ablation Studies</a></li>
                    <li><a href="#why-works">Why BERT Works</a></li>
                    <li><a href="#limitations">Limitations</a></li>
                </ul>
            </div>
        </nav>

        <!-- Article Content -->
        <article class="d-article">
            
            <!-- Intro note -->
            <p class="d-note">
                This article explains the landmark paper 
                <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> 
                by Devlin et al. (2018), which introduced a new paradigm for NLP: pre-train once, fine-tune everywhere.
            </p>

            <!-- Introduction -->
            <section id="introduction">
                <h2>Introduction</h2>
                <p>
                    In 2018, NLP faced a dilemma. Deep learning had revolutionized computer vision with 
                    ImageNet pre-training—train a big model on millions of images, then fine-tune for 
                    your specific task. But language didn't have an equivalent.
                </p>
                <p>Previous approaches fell into two camps:</p>
                <ul>
                    <li><strong>Feature-based (ELMo):</strong> Pre-train embeddings, freeze them, add task-specific architecture on top</li>
                    <li><strong>Fine-tuning (GPT):</strong> Pre-train a language model, fine-tune the whole thing—but only looking left-to-right</li>
                </ul>
                <p>
                    BERT unified and improved both: a single pre-trained model that could be fine-tuned 
                    for almost any NLP task, while capturing context from <em>both directions</em>.
                </p>
            </section>

            <!-- Directionality Problem -->
            <section id="directionality">
                <h2>The Directionality Problem</h2>

                <h3>Why direction matters</h3>
                <p>Consider the sentence:</p>
                
                <div class="d-example-box">
                    "The <strong>bank</strong> by the river was steep."
                </div>

                <p>To understand "bank" correctly, you need:</p>
                <ul>
                    <li><strong>Left context:</strong> "The" (not very helpful)</li>
                    <li><strong>Right context:</strong> "by the river" (critical—this means riverbank, not financial institution)</li>
                </ul>
                <p>
                    A left-to-right language model can't use the right context when processing "bank." 
                    It's fundamentally limited.
                </p>

                <h3>Previous solutions</h3>
                <p>
                    <strong>ELMo</strong> trained two separate LSTMs—one left-to-right, one right-to-left—and 
                    concatenated their outputs. This captures both directions, but the two directions 
                    don't interact during training.
                </p>
                <p>
                    <strong>GPT</strong> used a Transformer decoder, but it's autoregressive: each position 
                    can only attend to positions on its left. Powerful, but still unidirectional.
                </p>

                <div class="d-callout">
                    <strong>Key question:</strong> Can we train a single model where every position 
                    attends to every other position—truly bidirectional?
                </div>
            </section>

            <!-- Bidirectional Challenge -->
            <section id="bidirectional-challenge">
                <h2>The Bidirectional Challenge</h2>
                
                <p>The problem with "just make it bidirectional" is subtle but fundamental.</p>
                
                <p>In a standard language model, you predict the next word given previous words:</p>
                
                <div class="d-math-block">
                    $$P(w_t | w_1, w_2, \ldots, w_{t-1})$$
                </div>

                <p>
                    This is well-defined. Each word is predicted from context that doesn't include itself.
                </p>
                <p>
                    But if you allow bidirectional attention, each word can "see itself" through the 
                    other words. The model could trivially learn to copy. The training objective breaks down.
                </p>

                <div class="d-callout">
                    <strong>BERT's insight:</strong> Don't predict the next word. Predict <em>masked</em> words.
                </div>
            </section>

            <!-- Masked Language Modeling -->
            <section id="mlm">
                <h2>Masked Language Modeling (MLM)</h2>

                <p>The core pre-training objective of BERT is the <strong>Masked Language Model</strong>.</p>

                <h3>The procedure</h3>
                <ol>
                    <li>Take a sentence (or sentence pair)</li>
                    <li>Randomly select 15% of tokens to "mask"</li>
                    <li>Of those selected tokens:
                        <ul>
                            <li>80% → replace with <code>[MASK]</code> token</li>
                            <li>10% → replace with a random word</li>
                            <li>10% → keep unchanged</li>
                        </ul>
                    </li>
                    <li>Train the model to predict the original tokens</li>
                </ol>

                <figure class="d-figure">
                    <div class="d-figure-content">
                        <div class="bert-masking-viz">
                            <div class="masking-row">
                                <span class="masking-label">Input:</span>
                                <div class="masking-tokens">
                                    <span class="token">The</span>
                                    <span class="token">cat</span>
                                    <span class="token masked">[MASK]</span>
                                    <span class="token">on</span>
                                    <span class="token">the</span>
                                    <span class="token random">dog</span>
                                    <span class="token unchanged">.</span>
                                </div>
                            </div>
                            <div class="masking-row">
                                <span class="masking-label">Original:</span>
                                <div class="masking-tokens">
                                    <span class="token">The</span>
                                    <span class="token">cat</span>
                                    <span class="token target">sat</span>
                                    <span class="token">on</span>
                                    <span class="token">the</span>
                                    <span class="token target">mat</span>
                                    <span class="token target">.</span>
                                </div>
                            </div>
                            <div class="masking-legend">
                                <span class="legend-item"><span class="legend-box masked"></span> 80% [MASK]</span>
                                <span class="legend-item"><span class="legend-box random"></span> 10% random</span>
                                <span class="legend-item"><span class="legend-box unchanged"></span> 10% unchanged</span>
                            </div>
                        </div>
                    </div>
                    <figcaption class="d-figure-caption">
                        BERT's masking strategy. Of the 15% selected tokens, most become [MASK], 
                        some become random words, and some stay the same.
                    </figcaption>
                </figure>

                <h3>Why the 80/10/10 split?</h3>
                <p>
                    If we always used <code>[MASK]</code>, the model would never see real words in those 
                    positions during pre-training, but during fine-tuning there are no <code>[MASK]</code> 
                    tokens. This creates a mismatch.
                </p>
                <p>
                    The 10% random replacement teaches the model that it can't just trust every token 
                    it sees. The 10% unchanged teaches the model to use context even when the token 
                    looks "normal."
                </p>

                <h3>The equation</h3>
                <p>For each masked position $i$, BERT outputs a distribution over the vocabulary:</p>

                <div class="d-equation-panel">
                    <div class="d-equation-title">Masked Language Model Prediction</div>
                    <div class="d-equation-main">
                        $$P(w_i | \text{context}) = \text{softmax}(W_o \cdot h_i + b_o)$$
                    </div>
                    <div class="d-equation-legend">
                        <div class="d-legend-item">
                            <span class="d-legend-dot query"></span>
                            <span><strong>$h_i$</strong>: final hidden state at masked position</span>
                        </div>
                        <div class="d-legend-item">
                            <span class="d-legend-dot key"></span>
                            <span><strong>context</strong>: all other positions (bidirectional!)</span>
                        </div>
                        <div class="d-legend-item">
                            <span class="d-legend-dot param"></span>
                            <span><strong>$W_o, b_o$</strong>: output projection parameters</span>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Next Sentence Prediction -->
            <section id="nsp">
                <h2>Next Sentence Prediction (NSP)</h2>

                <p>
                    Many NLP tasks require understanding relationships between <em>sentences</em>, 
                    not just within them: question answering, natural language inference, etc.
                </p>
                <p>BERT adds a second pre-training objective: <strong>Next Sentence Prediction</strong>.</p>

                <h3>The procedure</h3>
                <ol>
                    <li>Sample sentence pairs (A, B) from the corpus</li>
                    <li>50% of the time: B is the actual next sentence after A (label: <code>IsNext</code>)</li>
                    <li>50% of the time: B is a random sentence (label: <code>NotNext</code>)</li>
                    <li>Train the model to classify the pair</li>
                </ol>

                <h3>Input format</h3>
                <p>BERT packs both sentences into a single sequence:</p>

                <figure class="d-figure">
                    <div class="d-figure-content">
                        <div class="bert-input-viz">
                            <div class="input-sequence">
                                <span class="token special">[CLS]</span>
                                <span class="token segment-a">tokens</span>
                                <span class="token segment-a">of</span>
                                <span class="token segment-a">sentence</span>
                                <span class="token segment-a">A</span>
                                <span class="token special">[SEP]</span>
                                <span class="token segment-b">tokens</span>
                                <span class="token segment-b">of</span>
                                <span class="token segment-b">sentence</span>
                                <span class="token segment-b">B</span>
                                <span class="token special">[SEP]</span>
                            </div>
                            <div class="input-legend">
                                <span class="legend-item"><span class="legend-box special"></span> Special tokens</span>
                                <span class="legend-item"><span class="legend-box segment-a"></span> Segment A</span>
                                <span class="legend-item"><span class="legend-box segment-b"></span> Segment B</span>
                            </div>
                        </div>
                    </div>
                    <figcaption class="d-figure-caption">
                        BERT input format. [CLS] provides the aggregate representation for classification. 
                        [SEP] separates the two sentences.
                    </figcaption>
                </figure>

                <p>
                    The <code>[CLS]</code> token's output becomes the aggregate sequence representation, 
                    used for sentence-level predictions.
                </p>
            </section>

            <!-- Architecture -->
            <section id="architecture">
                <h2>BERT Architecture</h2>

                <p>
                    BERT uses the <strong>Transformer encoder</strong> architecture—the same one from 
                    "Attention Is All You Need," but without the decoder.
                </p>

                <section id="model-sizes">
                    <h3>Model sizes</h3>
                    
                    <div class="d-table-wrapper">
                        <table class="d-table">
                            <thead>
                                <tr>
                                    <th></th>
                                    <th>BERT-Base</th>
                                    <th>BERT-Large</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Layers (L)</td>
                                    <td>12</td>
                                    <td>24</td>
                                </tr>
                                <tr>
                                    <td>Hidden size (H)</td>
                                    <td>768</td>
                                    <td>1024</td>
                                </tr>
                                <tr>
                                    <td>Attention heads (A)</td>
                                    <td>12</td>
                                    <td>16</td>
                                </tr>
                                <tr>
                                    <td>Parameters</td>
                                    <td>110M</td>
                                    <td>340M</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section id="input-repr">
                    <h3>Input representation</h3>
                    
                    <p>Each input token is represented as the sum of three embeddings:</p>

                    <div class="d-equation-panel">
                        <div class="d-equation-title">BERT Input Embedding</div>
                        <div class="d-equation-main">
                            $$\text{Input} = E_{\text{token}} + E_{\text{segment}} + E_{\text{position}}$$
                        </div>
                        <div class="d-equation-legend">
                            <div class="d-legend-item">
                                <span class="d-legend-dot query"></span>
                                <span><strong>Token embedding</strong>: WordPiece vocabulary of 30,000 tokens</span>
                            </div>
                            <div class="d-legend-item">
                                <span class="d-legend-dot key"></span>
                                <span><strong>Segment embedding</strong>: Which sentence (A or B) this token belongs to</span>
                            </div>
                            <div class="d-legend-item">
                                <span class="d-legend-dot value"></span>
                                <span><strong>Position embedding</strong>: Learned (not sinusoidal)</span>
                            </div>
                        </div>
                    </div>

                    <figure class="d-figure">
                        <div class="d-figure-content">
                            <div class="bert-embedding-viz">
                                <div class="embed-row">
                                    <span class="embed-label">Input:</span>
                                    <div class="embed-tokens">
                                        <span class="token">[CLS]</span>
                                        <span class="token">my</span>
                                        <span class="token">dog</span>
                                        <span class="token">is</span>
                                        <span class="token">cute</span>
                                        <span class="token">[SEP]</span>
                                        <span class="token">he</span>
                                        <span class="token">likes</span>
                                        <span class="token">play</span>
                                        <span class="token">##ing</span>
                                        <span class="token">[SEP]</span>
                                    </div>
                                </div>
                                <div class="embed-row">
                                    <span class="embed-label c-query">Token:</span>
                                    <div class="embed-tokens">
                                        <span class="embed-box query">E<sub>[CLS]</sub></span>
                                        <span class="embed-box query">E<sub>my</sub></span>
                                        <span class="embed-box query">E<sub>dog</sub></span>
                                        <span class="embed-box query">E<sub>is</sub></span>
                                        <span class="embed-box query">E<sub>cute</sub></span>
                                        <span class="embed-box query">E<sub>[SEP]</sub></span>
                                        <span class="embed-box query">E<sub>he</sub></span>
                                        <span class="embed-box query">E<sub>likes</sub></span>
                                        <span class="embed-box query">E<sub>play</sub></span>
                                        <span class="embed-box query">E<sub>##ing</sub></span>
                                        <span class="embed-box query">E<sub>[SEP]</sub></span>
                                    </div>
                                </div>
                                <div class="embed-row">
                                    <span class="embed-label c-key">Segment:</span>
                                    <div class="embed-tokens">
                                        <span class="embed-box key">E<sub>A</sub></span>
                                        <span class="embed-box key">E<sub>A</sub></span>
                                        <span class="embed-box key">E<sub>A</sub></span>
                                        <span class="embed-box key">E<sub>A</sub></span>
                                        <span class="embed-box key">E<sub>A</sub></span>
                                        <span class="embed-box key">E<sub>A</sub></span>
                                        <span class="embed-box key-b">E<sub>B</sub></span>
                                        <span class="embed-box key-b">E<sub>B</sub></span>
                                        <span class="embed-box key-b">E<sub>B</sub></span>
                                        <span class="embed-box key-b">E<sub>B</sub></span>
                                        <span class="embed-box key-b">E<sub>B</sub></span>
                                    </div>
                                </div>
                                <div class="embed-row">
                                    <span class="embed-label c-value">Position:</span>
                                    <div class="embed-tokens">
                                        <span class="embed-box value">E<sub>0</sub></span>
                                        <span class="embed-box value">E<sub>1</sub></span>
                                        <span class="embed-box value">E<sub>2</sub></span>
                                        <span class="embed-box value">E<sub>3</sub></span>
                                        <span class="embed-box value">E<sub>4</sub></span>
                                        <span class="embed-box value">E<sub>5</sub></span>
                                        <span class="embed-box value">E<sub>6</sub></span>
                                        <span class="embed-box value">E<sub>7</sub></span>
                                        <span class="embed-box value">E<sub>8</sub></span>
                                        <span class="embed-box value">E<sub>9</sub></span>
                                        <span class="embed-box value">E<sub>10</sub></span>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <figcaption class="d-figure-caption">
                            BERT's input is the sum of token, segment, and position embeddings. 
                            Note the WordPiece tokenization splitting "playing" into "play" + "##ing".
                        </figcaption>
                    </figure>
                </section>

                <h3>The computation</h3>
                <p>Stack of $L$ identical layers, each containing:</p>
                <ol>
                    <li><strong>Multi-head self-attention</strong> — every position attends to every position</li>
                    <li><strong>Feed-forward network</strong> — applied independently to each position</li>
                </ol>
                <p>Both wrapped with residual connections and layer normalization.</p>

                <div class="d-callout">
                    <strong>Key point:</strong> Unlike GPT, there's no masking in the attention. 
                    Position 5 can attend to position 10. This is what makes BERT bidirectional.
                </div>
            </section>

            <!-- Pre-training -->
            <section id="pretraining">
                <h2>Pre-training Data and Procedure</h2>

                <h3>Data</h3>
                <ul>
                    <li><strong>BooksCorpus</strong> (800M words): 11,038 unpublished books</li>
                    <li><strong>English Wikipedia</strong> (2,500M words): text only, no lists/tables/headers</li>
                </ul>
                <p>Total: ~3.3 billion words of pre-training data.</p>

                <h3>Training details</h3>
                <ul>
                    <li>Batch size: 256 sequences × 512 tokens = 131,072 tokens/batch</li>
                    <li>Training steps: 1,000,000 (about 40 epochs over the data)</li>
                    <li>Optimizer: Adam with learning rate warmup and linear decay</li>
                    <li>Hardware: 4 Cloud TPUs (16 TPU chips) for BERT-Base</li>
                    <li>Time: 4 days for BERT-Base</li>
                </ul>
            </section>

            <!-- Fine-tuning -->
            <section id="finetuning">
                <h2>Fine-tuning BERT</h2>

                <p>The breakthrough of BERT is how simple fine-tuning becomes. For most tasks:</p>
                <ol>
                    <li>Take the pre-trained BERT model</li>
                    <li>Add a single task-specific layer on top</li>
                    <li>Fine-tune <em>all</em> parameters on your labeled data (3-4 epochs)</li>
                </ol>

                <h3>Task adaptations</h3>

                <figure class="d-figure">
                    <div class="d-figure-content">
                        <div class="finetuning-grid">
                            <div class="finetune-task">
                                <div class="finetune-title">Sentence Classification</div>
                                <div class="finetune-example">(sentiment, topic)</div>
                                <div class="finetune-diagram">
                                    <div class="finetune-output">Softmax → Class</div>
                                    <div class="finetune-arrow">↑</div>
                                    <div class="finetune-bert">BERT</div>
                                    <div class="finetune-input">
                                        <span class="token special">[CLS]</span>
                                        <span class="token">sentence</span>
                                        <span class="token special">[SEP]</span>
                                    </div>
                                </div>
                            </div>
                            <div class="finetune-task">
                                <div class="finetune-title">Token Classification</div>
                                <div class="finetune-example">(NER, POS tagging)</div>
                                <div class="finetune-diagram">
                                    <div class="finetune-output-multi">
                                        <span>B-PER</span>
                                        <span>I-PER</span>
                                        <span>O</span>
                                    </div>
                                    <div class="finetune-arrow">↑ ↑ ↑</div>
                                    <div class="finetune-bert">BERT</div>
                                    <div class="finetune-input">
                                        <span class="token">John</span>
                                        <span class="token">Smith</span>
                                        <span class="token">works</span>
                                    </div>
                                </div>
                            </div>
                            <div class="finetune-task">
                                <div class="finetune-title">Question Answering</div>
                                <div class="finetune-example">(SQuAD)</div>
                                <div class="finetune-diagram">
                                    <div class="finetune-output">Start, End positions</div>
                                    <div class="finetune-arrow">↑</div>
                                    <div class="finetune-bert">BERT</div>
                                    <div class="finetune-input">
                                        <span class="token special">[CLS]</span>
                                        <span class="token">question</span>
                                        <span class="token special">[SEP]</span>
                                        <span class="token">passage</span>
                                        <span class="token special">[SEP]</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <figcaption class="d-figure-caption">
                        BERT fine-tuning for different tasks. The same pre-trained model adapts 
                        with minimal task-specific architecture.
                    </figcaption>
                </figure>

                <h3>Hyperparameters for fine-tuning</h3>
                <p>The authors found most tasks work well with:</p>
                <ul>
                    <li>Batch size: 16 or 32</li>
                    <li>Learning rate: 5e-5, 3e-5, or 2e-5</li>
                    <li>Epochs: 2, 3, or 4</li>
                    <li>Dropout: 0.1 (kept from pre-training)</li>
                </ul>
                <p>Fine-tuning is fast: minutes to hours on a single GPU for most datasets.</p>
            </section>

            <!-- Results -->
            <section id="results">
                <h2>Results</h2>

                <p>BERT achieved state-of-the-art on 11 NLP benchmarks at the time of publication.</p>

                <h3>GLUE Benchmark</h3>

                <div class="d-table-wrapper">
                    <table class="d-table">
                        <thead>
                            <tr>
                                <th>Task</th>
                                <th>Previous SOTA</th>
                                <th>BERT-Large</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>MNLI (accuracy)</td>
                                <td>80.6</td>
                                <td class="good"><strong>86.7</strong></td>
                            </tr>
                            <tr>
                                <td>QQP (F1)</td>
                                <td>66.1</td>
                                <td class="good"><strong>72.1</strong></td>
                            </tr>
                            <tr>
                                <td>QNLI (accuracy)</td>
                                <td>87.4</td>
                                <td class="good"><strong>92.7</strong></td>
                            </tr>
                            <tr>
                                <td>SST-2 (accuracy)</td>
                                <td>93.5</td>
                                <td class="good"><strong>94.9</strong></td>
                            </tr>
                            <tr>
                                <td>CoLA (Matthew's corr)</td>
                                <td>35.0</td>
                                <td class="good"><strong>60.5</strong></td>
                            </tr>
                            <tr class="highlight-row">
                                <td><strong>GLUE Average</strong></td>
                                <td>72.8</td>
                                <td class="good"><strong>80.5</strong></td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3>SQuAD (Question Answering)</h3>

                <div class="d-table-wrapper">
                    <table class="d-table">
                        <thead>
                            <tr>
                                <th></th>
                                <th>Human</th>
                                <th>Previous SOTA</th>
                                <th>BERT</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>SQuAD 1.1 (F1)</td>
                                <td>91.2</td>
                                <td>91.7</td>
                                <td class="good"><strong>93.2</strong></td>
                            </tr>
                            <tr>
                                <td>SQuAD 2.0 (F1)</td>
                                <td>89.5</td>
                                <td>78.0</td>
                                <td class="good"><strong>83.1</strong></td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p>
                    BERT exceeded human performance on SQuAD 1.1 and dramatically improved SQuAD 2.0, 
                    which includes unanswerable questions.
                </p>
            </section>

            <!-- Ablations -->
            <section id="ablations">
                <h2>Ablation Studies</h2>

                <p>The authors conducted careful ablations to understand what matters.</p>

                <h3>Pre-training objectives</h3>

                <div class="d-table-wrapper">
                    <table class="d-table">
                        <thead>
                            <tr>
                                <th>Configuration</th>
                                <th>MNLI</th>
                                <th>SQuAD</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="highlight-row">
                                <td>BERT (MLM + NSP)</td>
                                <td><strong>84.4</strong></td>
                                <td><strong>88.4</strong></td>
                            </tr>
                            <tr>
                                <td>No NSP</td>
                                <td>83.9</td>
                                <td>87.9</td>
                            </tr>
                            <tr>
                                <td>Left-to-right only</td>
                                <td class="bad">82.1</td>
                                <td class="bad">87.1</td>
                            </tr>
                            <tr>
                                <td>+ BiLSTM on top</td>
                                <td class="bad">82.1</td>
                                <td>87.7</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p><strong>Findings:</strong></p>
                <ul>
                    <li>NSP helps for tasks involving sentence pairs</li>
                    <li>Bidirectionality is crucial (left-to-right drops 2+ points)</li>
                    <li>A BiLSTM on top doesn't recover what's lost by not being bidirectional during pre-training</li>
                </ul>

                <div class="d-callout">
                    <strong>Key insight:</strong> Bigger is better—even for small datasets. A larger 
                    pre-trained model, fine-tuned on a small dataset, outperforms a smaller model 
                    fine-tuned on the same data.
                </div>
            </section>

            <!-- Why BERT Works -->
            <section id="why-works">
                <h2>Why BERT Works</h2>

                <p>Several factors contribute to BERT's success:</p>

                <h3>1. True bidirectionality</h3>
                <p>
                    Every position can attend to every other position. Information flows in all directions. 
                    This is more powerful than concatenating two unidirectional models.
                </p>

                <h3>2. Deep pre-training</h3>
                <p>
                    12-24 layers of Transformer, pre-trained on billions of words. The model learns rich 
                    representations of language structure, syntax, and semantics—all before seeing a 
                    single labeled example.
                </p>

                <h3>3. Simple fine-tuning</h3>
                <p>
                    No task-specific architecture needed. The same pre-trained model works for classification, 
                    tagging, and question answering. This democratized NLP: you no longer needed to design 
                    a new architecture for each task.
                </p>

                <h3>4. Shared representations</h3>
                <p>
                    The pre-trained representations capture general linguistic knowledge. Fine-tuning 
                    specializes them for a specific task, but starts from a strong foundation.
                </p>
            </section>

            <!-- Limitations -->
            <section id="limitations">
                <h2>Limitations and Trade-offs</h2>

                <div class="d-callout warning">
                    <strong>Computational cost:</strong> Pre-training BERT is expensive: days on TPUs, 
                    millions of training steps. Fine-tuning is cheap, but you need the pre-trained checkpoint.
                </div>

                <h3>Sequence length</h3>
                <p>
                    BERT is limited to 512 tokens due to memory constraints. For long documents, 
                    you need to truncate or use sliding windows.
                </p>

                <h3>The [MASK] token mismatch</h3>
                <p>
                    The <code>[MASK]</code> token appears during pre-training but not during fine-tuning. 
                    This pre-train/fine-tune mismatch may limit performance. (Later models like XLNet 
                    and ELECTRA address this.)
                </p>

                <h3>Not a language model</h3>
                <p>
                    BERT can't generate text autoregressively like GPT. It's designed for understanding, 
                    not generation. You can't just "sample from BERT."
                </p>
            </section>

            <!-- Legacy -->
            <section id="legacy">
                <h2>Legacy</h2>

                <p>BERT sparked an explosion of research:</p>
                <ul>
                    <li><strong>RoBERTa</strong> (2019): More data, no NSP, better hyperparameters</li>
                    <li><strong>ALBERT</strong> (2019): Parameter sharing for efficiency</li>
                    <li><strong>DistilBERT</strong> (2019): Smaller, faster, retains 97% of performance</li>
                    <li><strong>XLNet</strong> (2019): Permutation language modeling</li>
                    <li><strong>ELECTRA</strong> (2020): Replaced token detection instead of MLM</li>
                </ul>

                <p>
                    And the broader paradigm—<strong>pre-train, then fine-tune</strong>—became the default 
                    approach for NLP, eventually extending to GPT-3's few-shot learning and beyond.
                </p>

                <p>
                    BERT showed that with enough pre-training, a single model architecture could master 
                    almost any NLP task. That insight changed the field.
                </p>
            </section>

            <!-- Bibliography -->
            <section class="d-bibliography">
                <h2>References</h2>
                <ol>
                    <li>
                        Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). 
                        <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional 
                        Transformers for Language Understanding</a>. NAACL 2019.
                    </li>
                    <li>
                        Peters, M. E., et al. (2018). Deep contextualized word representations (ELMo). NAACL 2018.
                    </li>
                    <li>
                        Radford, A., et al. (2018). Improving Language Understanding by Generative Pre-Training (GPT). OpenAI.
                    </li>
                    <li>
                        Vaswani, A., et al. (2017). 
                        <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. NeurIPS 2017.
                    </li>
                </ol>
            </section>

            <!-- Appendix -->
            <footer class="d-appendix">
                <p>
                    This article is a Distill-style explanation of the BERT paper. 
                    <a href="https://arxiv.org/abs/1810.04805">Read the original paper →</a>
                </p>
            </footer>

        </article>
    </div>

    <script src="script.js"></script>
</body>
</html>
