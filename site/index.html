<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Knowledge Blog â€” Latest articles about machine learning</title>
    
    <meta name="description" content="Articles about Machine Learning">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="styles.css">
</head>
<body class="homepage">

    <!-- Header -->
    <header class="site-header">
        <div class="header-content l-page">
            <a href="index.html" class="logo">
                <svg viewBox="-607 419 64 64" class="logo-icon">
                    <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
                </svg>
                Knowledge Blog
            </a>
            <nav class="header-nav">
                <a href="#">About</a>
                <a href="#">Archive</a>
            </nav>
        </div>
    </header>

    <!-- Posts List -->
    <main class="posts-list l-page">
        
        <!-- Post 1: BERT (newest first) -->
        <article class="post-preview">
            <div class="post-meta">
                <span class="post-date">Feb. 2, 2026</span>
                <div class="post-tags">
                    <span class="tag explainer">Explainer</span>
                </div>
            </div>
            <a href="bert.html" class="post-link">
                <div class="post-thumbnail">
                    <div class="thumbnail-placeholder">
                        <svg viewBox="0 0 200 120" class="thumbnail-svg">
                            <!-- BERT bidirectional arrows -->
                            <rect x="70" y="40" width="60" height="40" rx="4" fill="#e3f2fd" stroke="#4a90a4" stroke-width="2"/>
                            <text x="100" y="65" text-anchor="middle" font-size="12" font-weight="600" fill="#1565c0">BERT</text>
                            <!-- Left arrows -->
                            <path d="M30,60 L60,60" stroke="#e07b39" stroke-width="2" marker-end="url(#arrow-orange)"/>
                            <path d="M60,60 L30,60" stroke="#7b68a4" stroke-width="2" marker-end="url(#arrow-purple)" transform="translate(0,10)"/>
                            <!-- Right arrows -->
                            <path d="M140,60 L170,60" stroke="#e07b39" stroke-width="2" marker-end="url(#arrow-orange)"/>
                            <path d="M170,60 L140,60" stroke="#7b68a4" stroke-width="2" marker-end="url(#arrow-purple)" transform="translate(0,10)"/>
                            <!-- Labels -->
                            <text x="30" y="55" font-size="8" fill="#666">context</text>
                            <text x="160" y="55" font-size="8" fill="#666">context</text>
                            <defs>
                                <marker id="arrow-orange" markerWidth="8" markerHeight="6" refX="7" refY="3" orient="auto">
                                    <polygon points="0 0, 8 3, 0 6" fill="#e07b39"/>
                                </marker>
                                <marker id="arrow-purple" markerWidth="8" markerHeight="6" refX="7" refY="3" orient="auto">
                                    <polygon points="0 0, 8 3, 0 6" fill="#7b68a4"/>
                                </marker>
                            </defs>
                        </svg>
                    </div>
                </div>
                <div class="post-content">
                    <h2 class="post-title">Understanding BERT</h2>
                    <p class="post-authors">Jacob&nbsp;Devlin, Ming-Wei&nbsp;Chang, Kenton&nbsp;Lee, Kristina&nbsp;Toutanova</p>
                    <p class="post-abstract">Pre-training deep bidirectional representations for language understanding. How masked language modeling enables a single model to master almost any NLP task.</p>
                </div>
            </a>
        </article>

        <!-- Post 2: Transformer -->
        <article class="post-preview">
            <div class="post-meta">
                <span class="post-date">Feb. 2, 2026</span>
                <div class="post-tags">
                    <span class="tag explainer">Explainer</span>
                </div>
            </div>
            <a href="transformer.html" class="post-link">
                <div class="post-thumbnail">
                    <div class="thumbnail-placeholder">
                        <svg viewBox="0 0 200 120" class="thumbnail-svg">
                            <!-- Simple Transformer diagram as placeholder -->
                            <rect x="20" y="30" width="60" height="60" fill="#fff3e0" stroke="#e07b39" stroke-width="2"/>
                            <rect x="120" y="30" width="60" height="60" fill="#e3f2fd" stroke="#4a90a4" stroke-width="2"/>
                            <line x1="80" y1="60" x2="120" y2="60" stroke="#666" stroke-width="2" marker-end="url(#arrow)"/>
                            <text x="50" y="65" text-anchor="middle" font-size="10" fill="#666">Encoder</text>
                            <text x="150" y="65" text-anchor="middle" font-size="10" fill="#666">Decoder</text>
                            <defs>
                                <marker id="arrow" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                    <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
                                </marker>
                            </defs>
                        </svg>
                    </div>
                </div>
                <div class="post-content">
                    <h2 class="post-title">Understanding the Transformer</h2>
                    <p class="post-authors">Ashish&nbsp;Vaswani, Noam&nbsp;Shazeer, Niki&nbsp;Parmar, <em>et&nbsp;al.</em></p>
                    <p class="post-abstract">Understanding the building blocks and design choices of the Transformer architecture that powers GPT, BERT, and modern language models.</p>
                </div>
            </a>
        </article>

    </main>

    <!-- Footer -->
    <footer class="site-footer l-page">
        <p>Knowledge Blog is dedicated to clear explanations of machine learning</p>
        <div class="footer-links">
            <a href="#">About</a>
            <a href="#">Archive</a>
            <a href="#">GitHub</a>
        </div>
    </footer>

</body>
</html>
